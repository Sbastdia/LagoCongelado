{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee619bf5-49ae-4084-bc6e-13d240c1c9e9",
   "metadata": {},
   "source": [
    "Primero instalamos el entorno de Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14088142-1bb2-4cff-91fa-5b0deebf2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gym matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e49e3-06cf-4140-869b-b57304575cef",
   "metadata": {},
   "source": [
    "Importamos las bibliotecas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c3c52f-4197-4917-8fb1-12673f642cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym #para el juego\n",
    "import random #para generar números aleatorios\n",
    "import numpy as np #para hacer algunos cálculos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969576d-d43d-47cf-b357-1d8343f1fbc6",
   "metadata": {},
   "source": [
    "Definición de estados:\n",
    "\n",
    "- S: starting point, safe\n",
    "- F: frozen surface, safe\n",
    "- H: hole, stuck forever\n",
    "- G: goal, safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22603dfb-f4ea-427c-a441-433dc61d3131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\x1b[41mS\\x1b[0mFFF\\nFHFH\\nFFFH\\nHFFG\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializar el entorno no resbaladizo de Frozen Lake\n",
    "environment = gym.make(\"FrozenLake-v1\",render_mode=\"ansi\", is_slippery=False)\n",
    "environment.reset()\n",
    "environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446cba1-8695-4531-848b-495ec04bca41",
   "metadata": {},
   "source": [
    "Tenemos 16 estados y 4 acciones, así que queremos calcular 64 valores.\n",
    "\n",
    "Vamos a crear nuestra tabla Q y llenarla con ceros ya que aún no tenemos idea del valor de cada acción en cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04da7274-2ecf-46ab-8b94-b6993b34fd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table =\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Inicializar Q-table con ceros\n",
    "# Nuestra \"table\" tiene las siguientes dimensiones:\n",
    "# (filas x columnas) = (estados x acciones) = (16 x 4)\n",
    "qtable = np.zeros((16, 4))\n",
    "\n",
    "# Alternativamente, la biblioteca \"gym\" también puede directamente darnos el número de estados y acciones usando \n",
    "# \"env.observation_space.n\" y \"env.action_space.n\"\n",
    "nb_states = environment.observation_space.n  # = 16\n",
    "nb_actions = environment.action_space.n      # = 4\n",
    "qtable = np.zeros((nb_states, nb_actions))\n",
    "\n",
    "# Veamos cómo se ve\n",
    "print('Q-table =')\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d35663-77b1-4051-a49f-4c8f65878beb",
   "metadata": {},
   "source": [
    "Tenemos nuestra tabla Q con 16 filas (nuestros 16 estados) y 4 columnas (nuestras 4 acciones) como esperábamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed928e8d-d710-4985-a636-397b112de368",
   "metadata": {},
   "source": [
    "Cada valor se establece en cero, por lo que no tenemos ninguna información. Digamos que el agente realiza una acción aleatoria. Podemos usar la biblioteca \"random\" con el método \"choice\" para elegir aleatoriamente una acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8ef77e8-e2f5-4123-9be0-abe43b0df1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DOWN'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice([\"LEFT\", \"DOWN\", \"RIGHT\", \"UP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8d865-c4ee-4174-ad2e-34fb3976852e",
   "metadata": {},
   "source": [
    "En realidad el agente se encuentra actualmente en el estado inicial S, lo que significa que solo son posibles dos acciones:\n",
    "- DERECHA y ABAJO\n",
    "- ARRIBA e IZQUIERDA\n",
    "pero no se moverá: su estado no cambia. Por lo tanto, no imponemos ninguna restricción sobre qué acciones son posibles: el agente comprenderá naturalmente que algunas de ellas no hacen nada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24bfae-2004-4886-80f4-d101d38c9558",
   "metadata": {},
   "source": [
    "Podemos seguir usando random.choice(), pero la biblioteca \"gym\" ya implementa un método para elegir aleatoriamente una acción. Podría ahorrarnos algunos problemas más adelante, así que lo hacemos así."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc97388f-e470-4e08-8e03-9e3de306c158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68748a3-f4d4-42e8-9755-01e81c1844be",
   "metadata": {},
   "source": [
    "Esta vez es un número. Los números se conectan con las direcciones de la siguiente manera:\n",
    "- LEFT: 0\n",
    "- DOWN: 1\n",
    "- RIGHT: 2\n",
    "- UP: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8d182-7ef6-48a4-b353-2a0c617e1864",
   "metadata": {},
   "source": [
    "Vamos a mover a nuestro agente a la derecha. Esta vez, se puede realizar utilizando el método \"step(action)\". Podemos probar a proporcionarle directamente el número 2, correspondiente a la dirección que elegimos (derecha), y comprobar si el agente se ha movido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3821b418-cdd0-468f-bef7-e4e9436185b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  (Right)\\nS\\x1b[41mF\\x1b[0mFF\\nFHFH\\nFFFH\\nHFFG\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.step(2)\n",
    "environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acc672-09e8-44be-b695-8ece7210cb94",
   "metadata": {},
   "source": [
    "Se ha movido desde el estado inicial S hacia la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4bac90-7394-422f-a4dc-f87e78b397b3",
   "metadata": {},
   "source": [
    "Lo que necesitamos saber para interactuar con el entorno es:\n",
    "- Cómo elegir aleatoriamente una acción usando action_space.sample()\n",
    "- Cómo implementar esta acción y mover a nuestro agente en la dirección deseada con step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4ed5f-24b2-40ca-9e79-2cf1bc60e0f5",
   "metadata": {},
   "source": [
    "Podemos agregar también:\n",
    "- Cómo mostrar el mapa actual para ver lo que estamos haciendo con render()\n",
    "- Cómo reiniciar el juego cuando el agente cae en un hoyo o llega a la meta G con reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3a65c-cc26-4306-a1fe-878012fa433f",
   "metadata": {},
   "source": [
    "Los agentes son recompensados por el entorno cuando logran un objetivo predefinido. En Frozen Lake, el agente solo es recompensado cuando alcanza el estado G. Esta recompensa no la podemos controlar, está configurada en el entorno: devuelve 1 cuando el agente llega a G, y 0 en caso contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f50f5-b4ff-494e-b773-ee93e5eb6ae6",
   "metadata": {},
   "source": [
    "Vamos a imprimirlo cada vez que implementemos una acción. La recompensa la da el método \"step(action)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de051d1d-f859-433e-bf1c-6483b4dee622",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vz/g15hkq995qg_xxn2dtvhp28w0000gn/T/ipykernel_7863/1640324295.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 2. Implementar esta acción y mover el agente en la dirección deseada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Mostramos los resultados (recompensa y mapa)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# 1. Elegir aleatoriamente una acción utilizando action_space.sample()\n",
    "action = environment.action_space.sample()\n",
    "\n",
    "# 2. Implementar esta acción y mover el agente en la dirección deseada\n",
    "new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "# Mostramos los resultados (recompensa y mapa)\n",
    "environment.render()\n",
    "print(f'Reward = {reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3cbabb-4a94-4645-9e6d-6e2b8195c2c4",
   "metadata": {},
   "source": [
    "Si alguna vez queremos ver una recompensa de 1, debemos tener la suerte de encontrar la secuencia correcta de acciones por casualidad. Así es exactamente como funciona, la tabla Q permanecerá llena de ceros hasta que el agente alcance aleatoriamente la meta G."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682fde0b-8194-4115-8a06-ccae7ee83301",
   "metadata": {},
   "source": [
    "El problema sería mucho más sencillo si pudiéramos tener recompensas intermedias más pequeñas para guiar nuestro camino hacia la meta G ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330c646-014c-4787-88cd-b7b577400780",
   "metadata": {},
   "source": [
    "Este es en realidad uno de los principales problemas del aprendizaje por refuerzo. Este fenómeno, llamado recompensas escasas, hace que sea muy difícil capacitar a los agentes en problemas en los que la única recompensa se encuentra al final de una larga secuencia de acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ebf67-1d80-4b16-9d26-74d9ce20ad51",
   "metadata": {},
   "source": [
    "¿Cómo retropropagar la información al estado inicial?\n",
    "- Necesitamos actualizar el valor de nuestros pares estado-acción (cada celda en la tabla Q) considerando 1: la recompensa por alcanzar el siguiente estado y 2: el valor más alto posible en el siguiente estado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0b714-5a90-4b44-a2eb-78bbe533b33e",
   "metadata": {},
   "source": [
    "Entonces entrenar a nuestro agente en código significa:\n",
    "- Elegir una acción aleatoria (usando action_space.sample()) si los valores en el estado actual son solo ceros. De lo contrario, tomamos la acción con el valor más alto en el estado actual con la función np.argmax()\n",
    "- Implementando esta acción moviéndose en la dirección deseada con step(action)\n",
    "- Actualizar el valor del estado original con la acción que realizamos, utilizando información sobre el nuevo estado y la recompensa otorgada por step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5b1862-ab25-4b89-8b45-ea7a72b5b6b9",
   "metadata": {},
   "source": [
    "Seguimos repitiendo estos 3 pasos hasta que el agente se atasca en un agujero o alcanza la meta G."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23755a-fcd7-48c8-bda7-841fb794378b",
   "metadata": {},
   "source": [
    "Podemos graficar el resultado de cada ejecución para observar el progreso de nuestro agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d4505b1-dad8-4ba6-8af7-cc71a98733dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vz/g15hkq995qg_xxn2dtvhp28w0000gn/T/ipykernel_7863/3777109507.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Elija la acción con el valor más alto en el estado actual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Importar matplotlib para trazar los resultados\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "\n",
    "# Reinicializamos la Q-table\n",
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hiperparámetros\n",
    "episodes = 1000        # Número total de episodios\n",
    "alpha = 0.5            # Tasa de aprendizaje\n",
    "gamma = 0.9            # Factor de descuento\n",
    "\n",
    "# Lista de resultados para trazar\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)\n",
    "\n",
    "# Entrenamiento\n",
    "for _ in range(episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # Por defecto, consideramos que nuestro resultado es un fracaso.\n",
    "    outcomes.append(\"Failure\")\n",
    "\n",
    "    # Hasta que el agente se quede atrapado en un hoyo o alcance la meta, sigue entrenándolo\n",
    "    while not done:\n",
    "        # Elija la acción con el valor más alto en el estado actual   \n",
    "        if np.max(qtable[state]) > 0:\n",
    "          action = np.argmax(qtable[state])\n",
    "\n",
    "        # Si no hay una mejor acción (solo ceros), elige una al azar\n",
    "        else:\n",
    "          action = environment.action_space.sample()\n",
    "             \n",
    "        # Implemente esta acción y mueva el agente en la dirección deseada.\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Actualizar Q(s, a)\n",
    "        qtable[state, action] = qtable[state, action] + \\\n",
    "                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "        \n",
    "        # Actualizar nuestro estado actual\n",
    "        state = new_state\n",
    "\n",
    "        # Si tenemos una recompensa, significa que nuestro resultado es un éxito.\n",
    "        if reward:\n",
    "          outcomes[-1] = \"Success\"\n",
    "\n",
    "print()\n",
    "print('===========================================')\n",
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "\n",
    "# Plot outcomes\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.xlabel(\"Run number\")\n",
    "plt.ylabel(\"Outcome\")\n",
    "ax = plt.gca()\n",
    "plt.bar(range(len(outcomes)), outcomes, width=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf0182-8799-4d9f-821c-d61ec2b281ab",
   "metadata": {},
   "source": [
    "Para ver cómo se desempeña el agente, podemos calcular el porcentaje de veces que logró alcanzar la meta (tasa de éxito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be63de3-fd0e-466f-8d10-0c1b0318a466",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vz/g15hkq995qg_xxn2dtvhp28w0000gn/T/ipykernel_7863/4225413940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Elija la acción con el valor más alto en el estado actual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Evaluación\n",
    "for _ in range(100):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Hasta que el agente se atasque o alcance la meta, sigue entrenándolo\n",
    "    while not done:\n",
    "        # Elija la acción con el valor más alto en el estado actual\n",
    "        if np.max(qtable[state]) > 0:\n",
    "          action = np.argmax(qtable[state])\n",
    "\n",
    "        # Si no hay una mejor acción (solo ceros), elegimos una al azar\n",
    "        else:\n",
    "          action = environment.action_space.sample()\n",
    "             \n",
    "        # Implementamos esta acción y movemos el agente en la dirección deseada.\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Actualizar nuestro estado actual\n",
    "        state = new_state\n",
    "\n",
    "        # Cuando recibimos una recompensa, significa que resolvimos el juego.\n",
    "        nb_success += reward\n",
    "        \n",
    "# ¡Revisemos nuestra tasa de éxito!\n",
    "print (f\"Success rate = {nb_success/episodes*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e61ba-61ce-4d52-afa7-63ba95e58260",
   "metadata": {},
   "source": [
    "Podemos visualizar al agente moviéndose en el mapa ejecutando el siguiente código e imprimir la secuencia de acciones que tomó para verificar si es la mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18a75883-251a-4d4c-8d35-1d836cdbc0e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vz/g15hkq995qg_xxn2dtvhp28w0000gn/T/ipykernel_7863/1499225699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Elija la acción con el valor más alto en el estado actual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time \n",
    "\n",
    "state = environment.reset()\n",
    "done = False\n",
    "sequence = []\n",
    "\n",
    "while not done:\n",
    "    # Elija la acción con el valor más alto en el estado actual\n",
    "    if np.max(qtable[state]) > 0:\n",
    "      action = np.argmax(qtable[state])\n",
    "\n",
    "    # Si no hay una mejor acción (solo ceros), elige una al azar\n",
    "    else:\n",
    "      action = environment.action_space.sample()\n",
    "    \n",
    "    # Añade la acción a la secuencia.\n",
    "    sequence.append(action)\n",
    "\n",
    "    # Implementamos esta acción y movemos el agente en la dirección deseada.\n",
    "    new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "    # Actualizar nuestro estado actual\n",
    "    state = new_state\n",
    "\n",
    "    # Actualizar el renderizado\n",
    "    clear_output(wait=True)\n",
    "    environment.render()\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Sequence = {sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe381bf-e546-4eee-bee2-dff37feb2442",
   "metadata": {},
   "source": [
    "Queremos permitir que nuestro agente:\n",
    "- Tome la acción con el valor más alto (explotación)\n",
    "- Elija una acción aleatoria para intentar encontrar otras aún mejores (exploración)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc472e40-60ac-4ea0-a9fe-3d7e60111867",
   "metadata": {},
   "source": [
    "Queremos una compensación entre estos dos comportamientos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bae4a6-a310-45b0-9ac3-b377f8479c29",
   "metadata": {},
   "source": [
    "Si el agente solo realiza acciones aleatorias, el entrenamiento no tiene sentido ya que no utiliza la tabla Q. Por eso queremos cambiar este parámetro con el tiempo: al comienzo del entrenamiento, queremos explorar el entorno tanto como sea posible. Pero la exploración se vuelve cada vez menos interesante, ya que el agente ya conoce todos los posibles pares estado-acción. Este parámetro representa la cantidad de aleatoriedad en la selección de acciones.\n",
    "\n",
    "Esta técnica se denomina comúnmente algoritmo ávido de épsilon , donde épsilon es nuestro parámetro. Es un método simple pero extremadamente eficiente para encontrar una buena compensación. Podemos disminuir el valor de épsilon al final de cada episodio en una cantidad fija (decaimiento lineal) o en función del valor actual de épsilon (decaimiento exponencial)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffabe8-0383-4fbf-a251-280b1d97399d",
   "metadata": {},
   "source": [
    "Implementemos un decaimiento lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9941a122-f76d-41c9-ba94-d07648152976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vz/g15hkq995qg_xxn2dtvhp28w0000gn/T/ipykernel_7863/830450438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Implemente esta acción y mueva el agente en la dirección deseada.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Actualizamos Q(s,a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Reinicializamos la Q-table\n",
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hiperparámetros\n",
    "episodes = 1000        # Número total de episodios\n",
    "alpha = 0.5            # Tasa de aprendizaje\n",
    "gamma = 0.9            # Factor de descuento\n",
    "epsilon = 1.0          # Cantidad de aleatoriedad en la selección de acciones.\n",
    "epsilon_decay = 0.001  # Cantidad fija a disminuir\n",
    "\n",
    "# Lista de resultados para trazar\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)\n",
    "\n",
    "# Entrenamiento\n",
    "for _ in range(episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # Por defecto, consideramos que nuestro resultado es un fracaso.\n",
    "    outcomes.append(\"Failure\")\n",
    "    \n",
    "    # Hasta que el agente se quede atrapado en un agujero o alcance la meta, sigue entrenándolo\n",
    "    while not done:\n",
    "        # Genera un número aleatorio entre 0 y 1\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        # Si el número aleatorio < épsilon, realiza una acción aleatoria\n",
    "        if rnd < epsilon:\n",
    "          action = environment.action_space.sample()\n",
    "        # De lo contrario, realice la acción con el valor más alto en el estado actual.\n",
    "        else:\n",
    "          action = np.argmax(qtable[state])\n",
    "             \n",
    "        # Implementamos esta acción y movemos el agente en la dirección deseada.\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Actualizamos Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + \\\n",
    "                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "        \n",
    "        # Actualizar nuestro estado actual\n",
    "        state = new_state\n",
    "\n",
    "        # Si tenemos una recompensa, significa que nuestro resultado es un éxito.\n",
    "        if reward:\n",
    "          outcomes[-1] = \"Success\"\n",
    "\n",
    "    # Actualizar épsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)\n",
    "\n",
    "print()\n",
    "print('===========================================')\n",
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "\n",
    "# Plot outcomes\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.xlabel(\"Run number\")\n",
    "plt.ylabel(\"Outcome\")\n",
    "ax = plt.gca()\n",
    "plt.bar(range(len(outcomes)), outcomes, width=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4567aa33-7ad1-4920-8351-92818538bf6b",
   "metadata": {},
   "source": [
    "El agente ha aprendido varias secuencias de acciones para alcanzar la meta. Es comprensible, ya que este nuevo agente se ve obligado a explorar pares estado-acción en lugar de explotar siempre los que tienen valores distintos de cero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09add7d1-b492-4ac7-90e7-562527682b70",
   "metadata": {},
   "source": [
    "A ver si tiene tanto éxito como el anterior para ganar la partida. En el modo de evaluación, ya no queremos exploración porque el agente ya está capacitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b91aa4ed-1ba3-4256-8229-36c799868764",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vz/g15hkq995qg_xxn2dtvhp28w0000gn/T/ipykernel_7863/329864357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Choose the action with the highest value in the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Implement this action and move the agent in the desired direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Evaluación\n",
    "for _ in range(100):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Hasta que el agente se atasque o alcance la meta, sigue entrenándolo\n",
    "    while not done:\n",
    "        # Elija la acción con el valor más alto en el estado actual\n",
    "        action = np.argmax(qtable[state])\n",
    "\n",
    "        # Implemente esta acción y mueva el agente en la dirección deseada.\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Actualizar nuestro estado actual\n",
    "        state = new_state\n",
    "\n",
    "        # Cuando recibimos una recompensa, significa que resolvimos el juego.\n",
    "        nb_success += reward\n",
    "\n",
    "# ¡Revisemos nuestra tasa de éxito!\n",
    "print (f\"Success rate = {nb_success/episodes*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8758b-e8e8-4931-8334-3ffa120010c8",
   "metadata": {},
   "source": [
    "No degradamos el modelo. Los beneficios de este enfoque pueden no ser obvios en este ejemplo, pero nuestro modelo se volvió menos estático y más flexible . Aprendió diferentes caminos (secuencias de acciones) de S a G en lugar de solo uno como en el enfoque anterior. Más exploración puede degradar el rendimiento, pero es necesario formar agentes que puedan adaptarse a nuevos entornos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a44d3-8075-4185-9e43-2a9bdee9b5f8",
   "metadata": {},
   "source": [
    "No resolvimos todo el entorno de Frozen Lake, solo entrenamos a un agente en la versión antideslizante, usando is_slippery = False durante la inicialización. Veamos qué tan bien le está yendo a nuestro código en este nuevo entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12c1ef88-bb4b-4e15-b4e9-85ada31f76e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vz/g15hkq995qg_xxn2dtvhp28w0000gn/T/ipykernel_7863/2543337579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Implemente esta acción y mueva el agente en la dirección deseada.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Actualizar Q(s,a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Inicializa el resbaladizo lago congelado\n",
    "environment = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "environment.reset()\n",
    "\n",
    "# Reinicializamos la Q-table\n",
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hiperparámetros\n",
    "episodes = 1000        # Total number of episodes\n",
    "alpha = 0.5            # Learning rate\n",
    "gamma = 0.9            # Discount factor\n",
    "epsilon = 1.0          # Amount of randomness in the action selection\n",
    "epsilon_decay = 0.001  # Fixed amount to decrease\n",
    "\n",
    "# Lista de resultados para trazar\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)\n",
    "\n",
    "# Capacitación\n",
    "for _ in range(episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # Por defecto, consideramos que nuestro resultado es un fracaso.\n",
    "    outcomes.append(\"Failure\")\n",
    "    \n",
    "    # Hasta que el agente se quede atrapado en un agujero o alcance la meta, sigue entrenándolo\n",
    "    while not done:\n",
    "        #Genera un número aleatorio entre 0 y 1\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        # Si el número aleatorio < épsilon, realiza una acción aleatoria\n",
    "        if rnd < epsilon:\n",
    "          action = environment.action_space.sample()\n",
    "        # De lo contrario, realice la acción con el valor más alto en el estado actual.\n",
    "        else:\n",
    "          action = np.argmax(qtable[state])\n",
    "             \n",
    "        # Implemente esta acción y mueva el agente en la dirección deseada.\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Actualizar Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + \\\n",
    "                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "        \n",
    "        # Actualizar nuestro estado actual\n",
    "        state = new_state\n",
    "\n",
    "        # Si tenemos una recompensa, significa que nuestro resultado es un éxito.\n",
    "        if reward:\n",
    "          outcomes[-1] = \"Success\"\n",
    "\n",
    "    # Actualizar épsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)\n",
    "\n",
    "print()\n",
    "print('===========================================')\n",
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "\n",
    "# Resultados de la trama\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.xlabel(\"Run number\")\n",
    "plt.ylabel(\"Outcome\")\n",
    "ax = plt.gca()\n",
    "plt.bar(range(len(outcomes)), outcomes, width=1.0)\n",
    "plt.show()\n",
    "\n",
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Evaluación\n",
    "for _ in range(100):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Hasta que el agente se atasque o alcance la meta, sigue entrenándolo\n",
    "    while not done:\n",
    "        # Elija la acción con el valor más alto en el estado actual\n",
    "        action = np.argmax(qtable[state])\n",
    "\n",
    "        # Implemente esta acción y mueva el agente en la dirección deseada.\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Actualizar nuestro estado actual\n",
    "        state = new_state\n",
    "\n",
    "        # Cuando recibimos una recompensa, significa que resolvimos el juego.\n",
    "        nb_success += reward\n",
    "\n",
    "# ¡Revisemos nuestra tasa de éxito!\n",
    "print (f\"Success rate = {nb_success/episodes*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4390e6e2-5fd1-4888-9050-93fef9917101",
   "metadata": {},
   "source": [
    "Modificar ligeramente los hiperparámetros puede destruir por completo los resultados. Esta es otra peculiaridad del aprendizaje por refuerzo, los hiperparámetros son bastante cambiantes y es importante comprender su significado si deseamos modificarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09f69c-2846-43b9-b628-52129849d17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
